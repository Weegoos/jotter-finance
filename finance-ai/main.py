from __future__ import annotations

import json
from typing import List, Optional

import httpx
from fastapi import Depends, FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel, Field

from config import Settings, get_settings
from llm_client import AlemLLMClient
from prompts import (
    PAIDA_SYSTEM_PROMPT,
    PAIDA_WELCOME_MESSAGE,
    THINKING_STEPS_PROMPT,
    CHAT_TOPIC_PROMPT,
)
from schemas import ChatMessage


# ============================================================================
# Request/Response Models
# ============================================================================


class ChatRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –∫ —á–∞—Ç—É pAIda."""

    message: str = Field(..., description="–°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", min_length=1)
    conversation_history: Optional[List[ChatMessage]] = Field(
        default=None,
        description="–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
    )
    conversation_id: Optional[str] = Field(
        default=None,
        description="UUID —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —á–∞—Ç–∞ (None = –Ω–æ–≤—ã–π —á–∞—Ç, —Ç–æ–ø–∏–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)",
    )
    model: Optional[str] = Field(default=None, description="–ú–æ–¥–µ–ª—å LLM")
    temperature: float = Field(default=0.7, ge=0, le=2)


class ChatResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –æ—Ç pAIda."""

    thinking_steps: List[str] = Field(
        ..., description="–®–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è ‚Äî —á—Ç–æ –¥–µ–ª–∞–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
    )
    message: str = Field(..., description="–û—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    model: str = Field(..., description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    conversation_id: Optional[str] = Field(
        default=None,
        description="UUID —á–∞—Ç–∞ (–ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –æ–±—Ä–∞—Ç–Ω–æ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞)",
    )
    generated_topic: Optional[str] = Field(
        default=None,
        description="–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–∞ (–µ—Å–ª–∏ generate_topic=True)",
    )


class KeyVerifyResponse(BaseModel):
    """Response model for API key verification."""

    provider: str
    status: str
    message: str
    details: Optional[dict] = None


class LLMTestRequest(BaseModel):
    """Request model for testing LLM API keys."""

    message: str = "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"
    max_tokens: int = 100
    temperature: float = 0.7


# ============================================================================
# Streaming Event Models
# ============================================================================


class StreamEventThinking(BaseModel):
    """–®–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è ‚Äî —á—Ç–æ –¥–µ–ª–∞–µ—Ç LLM (–∫–∞–∫ –≤ ChatGPT)."""

    type: str = "thinking"
    steps: List[str] = Field(..., description="–°–ø–∏—Å–æ–∫ —à–∞–≥–æ–≤ –º—ã—à–ª–µ–Ω–∏—è")


class StreamEventContent(BaseModel):
    """–¢–æ–∫–µ–Ω –æ—Ç–≤–µ—Ç–∞."""

    type: str = "content"
    delta: str


class StreamEventTopic(BaseModel):
    """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–ø–∏–∫ —á–∞—Ç–∞."""

    type: str = "topic"
    topic: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–ø–∏–∫–∞")
    conversation_id: Optional[str] = Field(default=None, description="UUID —á–∞—Ç–∞")


class StreamEventDone(BaseModel):
    """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""

    type: str = "done"
    status: str = "completed"


class StreamEventError(BaseModel):
    """–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""

    type: str = "error"
    error: str


# ============================================================================
# FastAPI App
# ============================================================================

app = FastAPI(
    title="Jotter Finance - pAIda API",
    description="–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç pAIda",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
)


@app.on_event("startup")
async def startup_event():
    settings = get_settings()
    app.state.settings = settings
    app.state.llm_client = AlemLLMClient(settings)


@app.on_event("shutdown")
async def shutdown_event():
    await app.state.llm_client.aclose()


app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:9000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def get_llm_client(request: Request) -> AlemLLMClient:
    return request.app.state.llm_client


def build_messages(
    user_message: str,
    conversation_history: Optional[List[ChatMessage]] = None,
) -> List[ChatMessage]:
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è LLM.
    –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç pAIda –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è.
    """
    messages = [ChatMessage(role="system", content=PAIDA_SYSTEM_PROMPT)]

    # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
    if conversation_history:
        for msg in conversation_history:
            if msg.role != "system":  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º system –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
                messages.append(ChatMessage(role=msg.role, content=msg.content))

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    messages.append(ChatMessage(role="user", content=user_message))

    return messages


def get_fallback_steps(user_message: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç fallback —à–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —è–∑—ã–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è."""
    has_cyrillic = any("\u0400" <= c <= "\u04ff" for c in user_message)
    has_kazakh = any(c in "”ô“ì“õ“£”©“±“Ø“ª—ñ" for c in user_message.lower())

    if has_kazakh:
        return [
            "–°“±—Ä–∞“õ—Ç—ã —Ç–∞–ª–¥–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω",
            "“ö–∞–∂–µ—Ç—Ç—ñ –∞“õ–ø–∞—Ä–∞—Ç—Ç—ã —ñ–∑–¥–µ–ø –∂–∞—Ç—ã—Ä–º—ã–Ω",
            "–ñ–∞—É–∞–ø—Ç—ã –¥–∞–π—ã–Ω–¥–∞–ø –∂–∞—Ç—ã—Ä–º—ã–Ω",
        ]
    elif has_cyrillic:
        return [
            "–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–≤–æ–π –∑–∞–ø—Ä–æ—Å",
            "–ü–æ–¥–±–∏—Ä–∞—é –Ω—É–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é",
            "–§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç",
        ]
    else:
        return [
            "Analyzing your request",
            "Gathering relevant information",
            "Preparing the response",
        ]


async def generate_thinking_steps(
    client: AlemLLMClient,
    user_message: str,
    model: str,
) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —à–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è (–∫–∞–∫ –≤ ChatGPT).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–∑ 3-5 —à–∞–≥–æ–≤.
    """
    fallback = get_fallback_steps(user_message)

    topic_prompt = THINKING_STEPS_PROMPT.format(user_message=user_message)

    messages = [
        ChatMessage(role="user", content=topic_prompt),
    ]

    try:
        result = await client.chat(
            messages,
            model=model,
            temperature=0.7,
        )
        response = result.get("message", "").strip()

        # –ü–∞—Ä—Å–∏–º —à–∞–≥–∏ (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ = –æ–¥–∏–Ω —à–∞–≥)
        steps = []
        for line in response.split("\n"):
            line = line.strip()
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –∏ –º–∞—Ä–∫–µ—Ä—ã
            line = line.lstrip("0123456789.-‚Äì‚Ä¢) ")
            line = line.strip("\"'`")

            if line and len(line) > 3 and len(line) < 100:
                steps.append(line)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 3-5 —à–∞–≥–æ–≤
        if len(steps) >= 3:
            return steps[:5]
        else:
            return fallback

    except Exception:
        return fallback


async def generate_chat_topic(
    client: AlemLLMClient,
    user_message: str,
    model: str,
) -> Optional[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ (—Ç–æ–ø–∏–∫) –¥–ª—è —á–∞—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É 2-5 —Å–ª–æ–≤ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    prompt = CHAT_TOPIC_PROMPT.format(message=user_message)

    messages = [
        ChatMessage(role="user", content=prompt),
    ]

    try:
        result = await client.chat(
            messages,
            model=model,
            temperature=0.5,  # –ù–∏–∂–µ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
        )
        topic = result.get("message", "").strip()

        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        topic = topic.strip('"\'`¬´¬ª‚Äû"')
        topic = topic.rstrip(".")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è: 2-50 —Å–∏–º–≤–æ–ª–æ–≤
        if topic and 2 <= len(topic) <= 50:
            return topic

        return None

    except Exception:
        return None


# ============================================================================
# Main Chat Endpoints
# ============================================================================


@app.post(
    "/llm/smart-chat",
    response_model=ChatResponse,
    summary="üí¨ –ß–∞—Ç —Å pAIda",
    description="""
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç pAIda.

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- –ü–æ–Ω–∏–º–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π, –∫–∞–∑–∞—Ö—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
- –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —è–∑—ã–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –æ–ø–µ—á–∞—Ç–∫–∏
- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–∞—Ö

**–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**
- "–ü—Ä–∏–≤–µ—Ç, –ø–æ–º–æ–≥–∏ —Å –±—é–¥–∂–µ—Ç–æ–º"
- "—Å–∫–æ–∫–∞ —è –ø–æ—Ç—Ä–∞—Ç–∏–ª –∑–∞ –º–µ—Å–µ—Ü" (—Å –æ–ø–µ—á–∞—Ç–∫–∞–º–∏)
- "–º–µ–Ω—ñ“£ —Ç–∞–±—ã—Å—ã–º “õ–∞–Ω—à–∞" (–∫–∞–∑–∞—Ö—Å–∫–∏–π)
- "how to save money" (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
""",
    tags=["Chat"],
)
async def chat_endpoint(
    request: ChatRequest,
    client: AlemLLMClient = Depends(get_llm_client),
    settings: Settings = Depends(get_settings),
):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç —á–∞—Ç–∞ —Å pAIda."""
    model = request.model or settings.primary_llm_model

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —à–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è
    thinking_steps = await generate_thinking_steps(client, request.message, model)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ø–∏–∫ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)
    generated_topic = None
    is_first_message = (
        not request.conversation_history or len(request.conversation_history) == 0
    )
    if is_first_message:
        generated_topic = await generate_chat_topic(client, request.message, model)

    messages = build_messages(request.message, request.conversation_history)

    try:
        result = await client.chat(
            messages,
            model=model,
            temperature=request.temperature,
        )
    except Exception as exc:
        raise HTTPException(status_code=502, detail=str(exc)) from exc

    return ChatResponse(
        thinking_steps=thinking_steps,
        message=result.get("message", ""),
        model=result.get("model", model),
        conversation_id=request.conversation_id,
        generated_topic=generated_topic,
    )


@app.post(
    "/llm/smart-chat/stream",
    summary="üí¨ –°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —á–∞—Ç —Å pAIda (SSE)",
    description="–û—Ç–≤–µ—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏, —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º.",
    tags=["Chat"],
)
async def chat_stream_endpoint(
    request: ChatRequest,
    client: AlemLLMClient = Depends(get_llm_client),
    settings: Settings = Depends(get_settings),
):
    """–°—Ç—Ä–∏–º–∏–Ω–≥–æ–≤—ã–π —á–∞—Ç —Å pAIda."""
    model = request.model or settings.primary_llm_model

    # –°–Ω–∞—á–∞–ª–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —à–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è
    thinking_steps = await generate_thinking_steps(client, request.message, model)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ø–∏–∫ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–Ω–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏)
    generated_topic = None
    is_first_message = (
        not request.conversation_history or len(request.conversation_history) == 0
    )
    if is_first_message:
        generated_topic = await generate_chat_topic(client, request.message, model)

    messages = build_messages(request.message, request.conversation_history)

    async def generate():
        try:
            # 1. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —à–∞–≥–∏ –º—ã—à–ª–µ–Ω–∏—è –ø–µ—Ä–≤—ã–º —Å–æ–±—ã—Ç–∏–µ–º
            thinking_event = StreamEventThinking(steps=thinking_steps)
            yield f"data: {thinking_event.model_dump_json()}\n\n"

            # 2. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–æ–ø–∏–∫ –µ—Å–ª–∏ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω
            if generated_topic:
                topic_event = StreamEventTopic(
                    topic=generated_topic,
                    conversation_id=request.conversation_id,
                )
                yield f"data: {topic_event.model_dump_json()}\n\n"

            # 3. –°—Ç—Ä–∏–º–∏–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            async for chunk in client.chat_stream(
                messages,
                model=model,
                temperature=request.temperature,
            ):
                try:
                    chunk_data = json.loads(chunk)
                    delta = (
                        chunk_data.get("choices", [{}])[0]
                        .get("delta", {})
                        .get("content", "")
                    )
                    if delta:
                        event = StreamEventContent(delta=delta)
                        yield f"data: {event.model_dump_json()}\n\n"
                except json.JSONDecodeError:
                    if chunk.strip():
                        event = StreamEventContent(delta=chunk)
                        yield f"data: {event.model_dump_json()}\n\n"

            # 4. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            done = StreamEventDone()
            yield f"data: {done.model_dump_json()}\n\n"

        except Exception as exc:
            error = StreamEventError(error=str(exc))
            yield f"data: {error.model_dump_json()}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


# ============================================================================
# Info & Health Endpoints
# ============================================================================


@app.get("/health", summary="Health check")
async def health():
    return {"status": "ok"}


@app.get("/paida/info", summary="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ pAIda", tags=["Info"])
async def get_paida_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–µ pAIda."""
    return {
        "name": "pAIda",
        "description": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç Jotter Finance",
        "welcome_message": PAIDA_WELCOME_MESSAGE,
        "capabilities": [
            "–ê–Ω–∞–ª–∏–∑ –ª–∏—á–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤",
            "–ë—é–¥–∂–µ—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–°–æ–≤–µ—Ç—ã –ø–æ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏—è–º",
            "–§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞—Å—Ö–æ–¥–∞–º–∏",
        ],
        "supported_languages": ["–†—É—Å—Å–∫–∏–π", "“ö–∞–∑–∞“õ—à–∞", "English"],
    }


# ============================================================================
# API Key Testing (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
# ============================================================================


async def _test_llm_key(
    base_url: str,
    api_key: str | None,
    provider: str,
    model: str,
    user_message: str,
    max_tokens: int = 100,
    temperature: float = 0.7,
) -> KeyVerifyResponse:
    """Helper to test LLM API keys."""
    if not api_key:
        return KeyVerifyResponse(
            provider=provider,
            status="error",
            message="‚ùå API key not configured",
        )

    try:
        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(
                f"{base_url.rstrip('/')}/chat/completions",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": user_message}],
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                },
            )

            if response.status_code == 200:
                data = response.json()
                assistant_message = (
                    data.get("choices", [{}])[0].get("message", {}).get("content", "")
                )
                return KeyVerifyResponse(
                    provider=provider,
                    status="success",
                    message="‚úÖ API key is valid",
                    details={"response": assistant_message[:200]},
                )
            else:
                return KeyVerifyResponse(
                    provider=provider,
                    status="error",
                    message=f"‚ùå Status {response.status_code}",
                )
    except Exception as e:
        return KeyVerifyResponse(
            provider=provider,
            status="error",
            message=f"‚ùå Error: {str(e)}",
        )


@app.post(
    "/test/alemllm",
    response_model=KeyVerifyResponse,
    summary="üß™ Test AlemLLM",
    tags=["Testing"],
)
async def test_alemllm_key(
    request: LLMTestRequest,
    settings: Settings = Depends(get_settings),
):
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ AlemLLM."""
    return await _test_llm_key(
        base_url=settings.alem_base_url,
        api_key=settings.alem_api_key,
        provider="AlemLLM",
        model="alemllm",
        user_message=request.message,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )


@app.exception_handler(Exception)
async def unhandled_exception_handler(_, exc: Exception):
    return JSONResponse(status_code=500, content={"detail": str(exc)})


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=2500)
